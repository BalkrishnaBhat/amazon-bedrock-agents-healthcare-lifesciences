UniProt and PDB Tools for Amazon Bedrock Agents

I'll create CloudFormation templates for UniProt and PDB tools that follow the same structure as the stock_data_stack example from the repository. These templates will define Lambda functions that can be directly used by Amazon Bedrock Agents, without API Gateway.
UniProt Tools CloudFormation Template

AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for UniProt Tools for Amazon Bedrock Agents'

Parameters:
  EmailForApiRequests:
    Type: String
    Description: Email address to use for API requests (good practice for UniProt API)
    Default: "your_email@example.com"
    
Resources:
  # IAM Role for Lambda functions
  UniProtLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'

  # Lambda function for searching proteins
  SearchProteinsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: search_proteins.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt UniProtLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting protein details
  GetProteinDetailsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_protein_details.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt UniProtLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting protein sequence
  GetProteinSequenceFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_protein_sequence.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt UniProtLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting protein features
  GetProteinFeaturesFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_protein_features.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt UniProtLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

Outputs:
  SearchProteinsFunction:
    Description: ARN of the SearchProteins Lambda function
    Value: !GetAtt SearchProteinsFunction.Arn
    
  GetProteinDetailsFunction:
    Description: ARN of the GetProteinDetails Lambda function
    Value: !GetAtt GetProteinDetailsFunction.Arn
    
  GetProteinSequenceFunction:
    Description: ARN of the GetProteinSequence Lambda function
    Value: !GetAtt GetProteinSequenceFunction.Arn
    
  GetProteinFeaturesFunction:
    Description: ARN of the GetProteinFeatures Lambda function
    Value: !GetAtt GetProteinFeaturesFunction.Arn

PDB Tools CloudFormation Template

AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for PDB Tools for Amazon Bedrock Agents'

Parameters:
  EmailForApiRequests:
    Type: String
    Description: Email address to use for API requests
    Default: "your_email@example.com"
    
Resources:
  # IAM Role for Lambda functions
  PdbLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'

  # Lambda function for searching protein structures
  SearchStructuresFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: search_structures.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt PdbLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting structure details
  GetStructureDetailsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_structure_details.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt PdbLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting structure summary
  GetStructureSummaryFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_structure_summary.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt PdbLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

  # Lambda function for getting ligand information
  GetLigandInformationFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: get_ligand_information.lambda_handler
      Runtime: python3.9
      CodeUri: ./functions/
      MemorySize: 256
      Timeout: 30
      Role: !GetAtt PdbLambdaRole.Arn
      Environment:
        Variables:
          EMAIL_ADDRESS: !Ref EmailForApiRequests

Outputs:
  SearchStructuresFunction:
    Description: ARN of the SearchStructures Lambda function
    Value: !GetAtt SearchStructuresFunction.Arn
    
  GetStructureDetailsFunction:
    Description: ARN of the GetStructureDetails Lambda function
    Value: !GetAtt GetStructureDetailsFunction.Arn
    
  GetStructureSummaryFunction:
    Description: ARN of the GetStructureSummary Lambda function
    Value: !GetAtt GetStructureSummaryFunction.Arn
    
  GetLigandInformationFunction:
    Description: ARN of the GetLigandInformation Lambda function
    Value: !GetAtt GetLigandInformationFunction.Arn

Implementation Files

Next, let me provide the individual Lambda function files for each service:
UniProt Functions
search_proteins.py

import json
import os
import requests
from urllib.parse import quote

def lambda_handler(event, context):
    try:
        # Parse input
        query = event.get('query')
        organism = event.get('organism')
        limit = event.get('limit', 10)
        
        if not query:
            return {
                'statusCode': 400,
                'body': 'Query parameter is required'
            }
        
        # Construct the search query
        search_term = query
        if organism:
            search_term += f" AND organism:{organism}"
        
        # Set up the request to UniProt API
        base_url = "https://rest.uniprot.org/uniprotkb/search"
        params = {
            'query': search_term,
            'format': 'json',
            'size': limit
        }
        
        headers = {
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Make the request
        response = requests.get(base_url, params=params, headers=headers)
        
        if response.status_code != 200:
            return {
                'statusCode': response.status_code,
                'body': f'UniProt API returned status code {response.status_code}'
            }
        
        # Process the results
        data = response.json()
        results = []
        
        for item in data.get('results', []):
            protein = {
                'id': item.get('primaryAccession'),
                'entryType': item.get('entryType'),
                'proteinName': None,
                'geneName': None,
                'organism': None,
                'reviewStatus': item.get('entryType')
            }
            
            # Extract protein name
            protein_data = item.get('protein', {})
            if 'recommendedName' in protein_data:
                protein['proteinName'] = protein_data['recommendedName'].get('fullName', {}).get('value')
            elif 'submittedName' in protein_data and len(protein_data.get('submittedName', [])) > 0:
                protein['proteinName'] = protein_data['submittedName'][0].get('fullName', {}).get('value')
            
            # Extract gene name
            genes = item.get('genes', [])
            if genes and len(genes) > 0 and 'geneName' in genes[0]:
                protein['geneName'] = genes[0]['geneName'].get('value')
            
            # Extract organism information
            organism_data = item.get('organism', {})
            if organism_data:
                protein['organism'] = organism_data.get('scientificName')
            
            results.append(protein)
        
        return {
            'query': query,
            'results': results,
            'totalResults': data.get('numberOfHits', 0)
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_protein_details.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        protein_id = event.get('protein_id')
        
        if not protein_id:
            return {
                'statusCode': 400,
                'body': 'protein_id parameter is required'
            }
        
        # Set up the request to UniProt API
        base_url = f"https://rest.uniprot.org/uniprotkb/{protein_id}"
        params = {
            'format': 'json'
        }
        
        headers = {
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Make the request
        response = requests.get(base_url, params=params, headers=headers)
        
        if response.status_code == 404:
            return {
                'statusCode': 404,
                'body': f'Protein with ID {protein_id} not found'
            }
            
        if response.status_code != 200:
            return {
                'statusCode': response.status_code,
                'body': f'UniProt API returned status code {response.status_code}'
            }
        
        # Process the result
        data = response.json()
        
        # Extract key information
        result = {
            'id': data.get('primaryAccession'),
            'entryType': data.get('entryType'),
            'proteinName': None,
            'geneName': None,
            'organism': None,
            'function': None,
            'sequenceLength': data.get('sequence', {}).get('length'),
            'keywords': [],
            'diseases': [],
            'subcellularLocations': []
        }
        
        # Extract protein name
        protein_data = data.get('protein', {})
        if 'recommendedName' in protein_data:
            result['proteinName'] = protein_data['recommendedName'].get('fullName', {}).get('value')
        elif 'submittedName' in protein_data and len(protein_data.get('submittedName', [])) > 0:
            result['proteinName'] = protein_data['submittedName'][0].get('fullName', {}).get('value')
        
        # Extract gene name
        genes = data.get('genes', [])
        if genes and len(genes) > 0 and 'geneName' in genes[0]:
            result['geneName'] = genes[0]['geneName'].get('value')
        
        # Extract organism information
        organism_data = data.get('organism', {})
        if organism_data:
            result['organism'] = organism_data.get('scientificName')
        
        # Extract function
        comments = data.get('comments', [])
        for comment in comments:
            if comment.get('commentType') == 'FUNCTION':
                result['function'] = comment.get('texts', [{}])[0].get('value')
                break
        
        # Extract keywords
        keywords = data.get('keywords', [])
        result['keywords'] = [k.get('name') for k in keywords if 'name' in k]
        
        # Extract disease information
        for comment in comments:
            if comment.get('commentType') == 'DISEASE':
                disease = comment.get('disease', {})
                if disease:
                    result['diseases'].append({
                        'name': disease.get('diseaseId'),
                        'description': disease.get('description')
                    })
        
        # Extract subcellular location
        for comment in comments:
            if comment.get('commentType') == 'SUBCELLULAR LOCATION':
                for location in comment.get('subcellularLocations', []):
                    if 'location' in location:
                        result['subcellularLocations'].append(location['location'].get('value'))
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_protein_sequence.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        protein_id = event.get('protein_id')
        
        if not protein_id:
            return {
                'statusCode': 400,
                'body': 'protein_id parameter is required'
            }
        
        # First, check if the protein exists
        base_url = f"https://rest.uniprot.org/uniprotkb/{protein_id}"
        
        headers = {
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Make the initial request to check existence
        response = requests.get(base_url, headers=headers, params={'format': 'json'})
        
        if response.status_code == 404:
            return {
                'statusCode': 404,
                'body': f'Protein with ID {protein_id} not found'
            }
            
        if response.status_code != 200:
            return {
                'statusCode': response.status_code,
                'body': f'UniProt API returned status code {response.status_code}'
            }
        
        # Get basic protein info from the response
        data = response.json()
        protein_name = None
        
        # Extract protein name
        protein_data = data.get('protein', {})
        if 'recommendedName' in protein_data:
            protein_name = protein_data['recommendedName'].get('fullName', {}).get('value')
        elif 'submittedName' in protein_data and len(protein_data.get('submittedName', [])) > 0:
            protein_name = protein_data['submittedName'][0].get('fullName', {}).get('value')
        
        # Now get the sequence in FASTA format
        fasta_url = f"https://rest.uniprot.org/uniprotkb/{protein_id}.fasta"
        fasta_response = requests.get(fasta_url, headers=headers)
        
        if fasta_response.status_code != 200:
            return {
                'statusCode': fasta_response.status_code,
                'body': f'Failed to retrieve sequence. Status code: {fasta_response.status_code}'
            }
        
        # Process the FASTA response
        fasta_content = fasta_response.text
        lines = fasta_content.strip().split('\n')
        
        # First line is the header, rest is the sequence
        header = lines[0]
        sequence = ''.join(lines[1:])
        
        result = {
            'protein_id': protein_id,
            'proteinName': protein_name,
            'sequenceLength': len(sequence),
            'sequence': sequence
        }
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_protein_features.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        protein_id = event.get('protein_id')
        
        if not protein_id:
            return {
                'statusCode': 400,
                'body': 'protein_id parameter is required'
            }
        
        # Set up the request to UniProt API
        base_url = f"https://rest.uniprot.org/uniprotkb/{protein_id}"
        params = {
            'format': 'json'
        }
        
        headers = {
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Make the request
        response = requests.get(base_url, params=params, headers=headers)
        
        if response.status_code == 404:
            return {
                'statusCode': 404,
                'body': f'Protein with ID {protein_id} not found'
            }
            
        if response.status_code != 200:
            return {
                'statusCode': response.status_code,
                'body': f'UniProt API returned status code {response.status_code}'
            }
        
        # Process the result
        data = response.json()
        
        # Extract features
        features = data.get('features', [])
        
        # Categorize features
        categorized_features = {}
        
        for feature in features:
            feature_type = feature.get('type')
            
            if feature_type not in categorized_features:
                categorized_features[feature_type] = []
                
            feature_info = {
                'description': feature.get('description'),
                'location': {
                    'start': feature.get('location', {}).get('start', {}).get('value'),
                    'end': feature.get('location', {}).get('end', {}).get('value')
                }
            }
            
            if 'evidences' in feature:
                feature_info['evidenceType'] = [e.get('evidenceCode') for e in feature.get('evidences', [])]
                
            categorized_features[feature_type].append(feature_info)
        
        # Get basic protein info
        protein_name = None
        protein_data = data.get('protein', {})
        if 'recommendedName' in protein_data:
            protein_name = protein_data['recommendedName'].get('fullName', {}).get('value')
        elif 'submittedName' in protein_data and len(protein_data.get('submittedName', [])) > 0:
            protein_name = protein_data['submittedName'][0].get('fullName', {}).get('value')
        
        # Prepare the result
        result = {
            'protein_id': protein_id,
            'proteinName': protein_name,
            'organism': data.get('organism', {}).get('scientificName'),
            'features': categorized_features
        }
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

PDB Functions
search_structures.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        query = event.get('query')
        author = event.get('author')
        exp_method = event.get('experiment_method')  # e.g., "X-RAY", "NMR", "EM"
        resolution = event.get('resolution')  # e.g., "2.5" (less than 2.5 Å)
        limit = event.get('limit', 10)
        
        if not any([query, author, exp_method]):
            return {
                'statusCode': 400,
                'body': 'At least one search parameter is required (query, author, or experiment_method)'
            }
        
        # Construct the search query
        search_url = "https://search.rcsb.org/rcsbsearch/v2/query"
        
        # Start building the query
        query_params = []
        
        # Add text search if provided
        if query:
            query_params.append({
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "rcsb_entry_info.structure_title",
                    "operator": "contains_words",
                    "value": query
                }
            })
        
        # Add author search if provided
        if author:
            query_params.append({
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "rcsb_primary_citation.rcsb_authors",
                    "operator": "contains_words",
                    "value": author
                }
            })
        
        # Add experimental method filter if provided
        if exp_method:
            query_params.append({
                "type": "terminal",
                "service": "text",
                "parameters": {
                    "attribute": "exptl.method",
                    "operator": "exact_match",
                    "value": exp_method.upper()
                }
            })
        
        # Add resolution filter if provided
        if resolution:
            try:
                resolution_float = float(resolution)
                query_params.append({
                    "type": "terminal",
                    "service": "text",
                    "parameters": {
                        "attribute": "rcsb_entry_info.resolution_combined",
                        "operator": "less_or_equal",
                        "value": resolution_float
                    }
                })
            except ValueError:
                # Ignore resolution if it's not a valid float
                pass
        
        # Combine all query parameters with AND
        if len(query_params) > 1:
            final_query = {
                "type": "group",
                "logical_operator": "and",
                "nodes": query_params
            }
        else:
            final_query = query_params[0]
        
        # Prepare the full request
        request_body = {
            "query": final_query,
            "return_type": "entry",
            "request_options": {
                "paginate": {
                    "start": 0,
                    "rows": limit
                }
            }
        }
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Make the request
        response = requests.post(search_url, json=request_body, headers=headers)
        
        if response.status_code != 200:
            return {
                'statusCode': response.status_code,
                'body': f'PDB API returned status code {response.status_code}: {response.text}'
            }
        
        # Process the results
        result_data = response.json()
        total_count = result_data.get('total_count', 0)
        result_ids = [item['identifier'] for item in result_data.get('result_set', [])]
        
        # If we got results, get more details for each structure
        detailed_results = []
        
        if result_ids:
            # Get summary information for each PDB ID
            summary_url = "https://data.rcsb.org/graphql"
            
            # Prepare the GraphQL query
            graphql_query = """
            query(\$ids: [String!]!) {
              entries(entry_ids: \$ids) {
                rcsb_id
                struct {
                  title
                }
                rcsb_primary_citation {
                  rcsb_authors
                  title
                  journal_abbrev
                  year
                }
                exptl {
                  method
                }
                rcsb_entry_info {
                  resolution_combined
                  deposited_atom_count
                  polymer_entity_count_protein
                  release_date
                }
              }
            }
            """
            
            # Make the GraphQL request
            graphql_response = requests.post(
                summary_url,
                json={"query": graphql_query, "variables": {"ids": result_ids}},
                headers=headers
            )
            
            if graphql_response.status_code == 200:
                entries_data = graphql_response.json().get('data', {}).get('entries', [])
                
                for entry in entries_data:
                    structure_info = {
                        'pdb_id': entry.get('rcsb_id'),
                        'title': entry.get('struct', {}).get('title'),
                        'method': entry.get('exptl', [{}])[0].get('method') if entry.get('exptl') else None,
                        'resolution': entry.get('rcsb_entry_info', {}).get('resolution_combined'),
                        'release_date': entry.get('rcsb_entry_info', {}).get('release_date'),
                        'protein_entities': entry.get('rcsb_entry_info', {}).get('polymer_entity_count_protein'),
                        'atom_count': entry.get('rcsb_entry_info', {}).get('deposited_atom_count')
                    }
                    
                    # Add citation information if available
                    citation = entry.get('rcsb_primary_citation', {})
                    if citation:
                        authors = citation.get('rcsb_authors', [])
                        # Limit to first 3 authors for brevity
                        if len(authors) > 3:
                            authors = authors[:3]
                            authors.append("et al.")
                            
                        structure_info['citation'] = {
                            'authors': authors,
                            'title': citation.get('title'),
                            'journal': citation.get('journal_abbrev'),
                            'year': citation.get('year')
                        }
                    
                    detailed_results.append(structure_info)
        
        return {
            'query': query,
            'total_count': total_count,
            'results': detailed_results
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_structure_details.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        pdb_id = event.get('pdb_id', '').strip().upper()
        
        if not pdb_id:
            return {
                'statusCode': 400,
                'body': 'pdb_id parameter is required'
            }
        
        # Validate PDB ID format (typically 4 characters)
        if len(pdb_id) != 4:
            return {
                'statusCode': 400,
                'body': 'Invalid PDB ID format. Expected 4 characters.'
            }
        
        # Set up the request to RCSB PDB API
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Use GraphQL to get comprehensive information
        url = "https://data.rcsb.org/graphql"
        
        # Prepare the GraphQL query to get detailed information
        graphql_query = """
        query(\$pdb_id: String!) {
          entry(entry_id: \$pdb_id) {
            rcsb_id
            struct {
              title
              pdbx_descriptor
            }
            rcsb_primary_citation {
              rcsb_authors
              title
              journal_abbrev
              year
              pdbx_database_id_DOI
            }
            exptl {
              method
            }
            rcsb_entry_info {
              resolution_combined
              deposited_model_count
              deposited_atom_count
              polymer_entity_count_protein
              polymer_entity_count_nucleic_acid
              polymer_entity_count_carbohydrate
              nonpolymer_entity_count
              molecular_weight
              release_date
              revision_date
              experimental_method
              experimental_method_count
              structure_determination_methodology
            }
            audit_author {
              name
            }
            struct_keywords {
              pdbx_keywords
              text
            }
            entity {
              polymer_type
              rcsb_entity_source_organism {
                scientific_name
                common_name
                ncbi_taxonomy_id
              }
              rcsb_description {
                description
              }
            }
            pdbx_database_related {
              db_name
              content_type
              db_id
            }
            reflns {
              d_resolution_high
              percent_possible_obs
            }
          }
        }
        """
        
        # Make the GraphQL request
        graphql_response = requests.post(
            url,
            json={"query": graphql_query, "variables": {"pdb_id": pdb_id}},
            headers=headers
        )
        
        if graphql_response.status_code != 200:
            return {
                'statusCode': graphql_response.status_code,
                'body': f'PDB API returned status code {graphql_response.status_code}: {graphql_response.text}'
            }
        
        data = graphql_response.json()
        entry_data = data.get('data', {}).get('entry')
        
        if not entry_data:
            return {
                'statusCode': 404,
                'body': f'PDB ID {pdb_id} not found'
            }
        
        # Extract entities information
        entities = entry_data.get('entity', [])
        organism_names = set()
        entity_descriptions = []
        
        for entity in entities:
            # Get organism names
            source_organisms = entity.get('rcsb_entity_source_organism', [])
            for organism in source_organisms:
                if 'scientific_name' in organism:
                    organism_names.add(organism['scientific_name'])
            
            # Get entity descriptions
            descriptions = entity.get('rcsb_description', [])
            for desc in descriptions:
                if 'description' in desc:
                    entity_descriptions.append(desc['description'])
        
        # Format author list
        authors = [author['name'] for author in entry_data.get('audit_author', [])]
        
        # Format citation
        citation = entry_data.get('rcsb_primary_citation', {})
        citation_text = ""
        if citation:
            citation_authors = citation.get('rcsb_authors', [])
            if len(citation_authors) > 3:
                author_text = f"{', '.join(citation_authors[:3])} et al."
            else:
                author_text = ', '.join(citation_authors)
            
            citation_year = citation.get('year')
            citation_title = citation.get('title')
            citation_journal = citation.get('journal_abbrev')
            
            if all([author_text, citation_year, citation_title, citation_journal]):
                citation_text = f"{author_text} ({citation_year}). {citation_title}. {citation_journal}"
        
        # Format result data
        result = {
            'pdb_id': pdb_id,
            'title': entry_data.get('struct', {}).get('title'),
            'description': entry_data.get('struct', {}).get('pdbx_descriptor'),
            'keywords': entry_data.get('struct_keywords', {}).get('pdbx_keywords'),
            'experimental_method': entry_data.get('rcsb_entry_info', {}).get('experimental_method'),
            'resolution': entry_data.get('rcsb_entry_info', {}).get('resolution_combined'),
            'deposition_date': entry_data.get('rcsb_entry_info', {}).get('release_date'),
            'revision_date': entry_data.get('rcsb_entry_info', {}).get('revision_date'),
            'atom_count': entry_data.get('rcsb_entry_info', {}).get('deposited_atom_count'),
            'authors': authors,
            'organisms': list(organism_names),
            'entity_descriptions': entity_descriptions,
            'citation': citation_text,
            'doi': citation.get('pdbx_database_id_DOI'),
            'related_entries': [
                {
                    'db_name': rel.get('db_name'),
                    'content_type': rel.get('content_type'),
                    'id': rel.get('db_id')
                }
                for rel in entry_data.get('pdbx_database_related', [])
            ]
        }
        
        # Add links to 3D viewers and download files
        result['links'] = {
            '3d_viewer': f"https://www.rcsb.org/3d-view/{pdb_id}",
            'pdb_file': f"https://files.rcsb.org/download/{pdb_id}.pdb",
            'cif_file': f"https://files.rcsb.org/download/{pdb_id}.cif",
            'biological_assembly': f"https://files.rcsb.org/download/{pdb_id}.pdb1",
            'rcsb_page': f"https://www.rcsb.org/structure/{pdb_id}"
        }
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_structure_summary.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        pdb_id = event.get('pdb_id', '').strip().upper()
        
        if not pdb_id:
            return {
                'statusCode': 400,
                'body': 'pdb_id parameter is required'
            }
        
        # Validate PDB ID format (typically 4 characters)
        if len(pdb_id) != 4:
            return {
                'statusCode': 400,
                'body': 'Invalid PDB ID format. Expected 4 characters.'
            }
        
        # Set up the request to RCSB PDB API
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Use GraphQL to get summary information
        url = "https://data.rcsb.org/graphql"
        
        # Prepare a simplified GraphQL query for summary
        graphql_query = """
        query(\$pdb_id: String!) {
          entry(entry_id: \$pdb_id) {
            rcsb_id
            struct {
              title
            }
            rcsb_entry_info {
              experimental_method
              resolution_combined
              polymer_entity_count_protein
              nonpolymer_entity_count
              release_date
            }
            exptl {
              method
            }
            entity {
              polymer_type
              rcsb_entity_source_organism {
                scientific_name
              }
            }
            struct_keywords {
              text
            }
          }
        }
        """
        
        # Make the GraphQL request
        graphql_response = requests.post(
            url,
            json={"query": graphql_query, "variables": {"pdb_id": pdb_id}},
            headers=headers
        )
        
        if graphql_response.status_code != 200:
            return {
                'statusCode': graphql_response.status_code,
                'body': f'PDB API returned status code {graphql_response.status_code}: {graphql_response.text}'
            }
        
        data = graphql_response.json()
        entry_data = data.get('data', {}).get('entry')
        
        if not entry_data:
            return {
                'statusCode': 404,
                'body': f'PDB ID {pdb_id} not found'
            }
        
        # Extract organisms
        organisms = set()
        entities = entry_data.get('entity', [])
        for entity in entities:
            source_organisms = entity.get('rcsb_entity_source_organism', [])
            for organism in source_organisms:
                if 'scientific_name' in organism:
                    organisms.add(organism['scientific_name'])
        
        # Extract experimental method
        method = entry_data.get('exptl', [{}])[0].get('method') if entry_data.get('exptl') else None
        
        # Format result data
        result = {
            'pdb_id': pdb_id,
            'title': entry_data.get('struct', {}).get('title'),
            'experimental_method': method or entry_data.get('rcsb_entry_info', {}).get('experimental_method'),
            'resolution': entry_data.get('rcsb_entry_info', {}).get('resolution_combined'),
            'protein_entities': entry_data.get('rcsb_entry_info', {}).get('polymer_entity_count_protein'),
            'ligand_count': entry_data.get('rcsb_entry_info', {}).get('nonpolymer_entity_count'),
            'release_date': entry_data.get('rcsb_entry_info', {}).get('release_date'),
            'organisms': list(organisms),
            'keywords': entry_data.get('struct_keywords', {}).get('text')
        }
        
        # Add links
        result['links'] = {
            'rcsb_page': f"https://www.rcsb.org/structure/{pdb_id}",
            '3d_viewer': f"https://www.rcsb.org/3d-view/{pdb_id}"
        }
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

get_ligand_information.py

import json
import os
import requests

def lambda_handler(event, context):
    try:
        # Parse input
        pdb_id = event.get('pdb_id', '').strip().upper()
        
        if not pdb_id:
            return {
                'statusCode': 400,
                'body': 'pdb_id parameter is required'
            }
        
        # Validate PDB ID format (typically 4 characters)
        if len(pdb_id) != 4:
            return {
                'statusCode': 400,
                'body': 'Invalid PDB ID format. Expected 4 characters.'
            }
        
        # Set up the request to RCSB PDB API
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': f'Python script/1.0 ({os.environ.get("EMAIL_ADDRESS", "anonymous")})'
        }
        
        # Use GraphQL to get ligand information
        url = "https://data.rcsb.org/graphql"
        
        # Prepare the GraphQL query to get ligand details
        graphql_query = """
        query(\$pdb_id: String!) {
          entry(entry_id: \$pdb_id) {
            rcsb_id
            struct {
              title
            }
            nonpolymer_entity {
              pdbx_description
              pdbx_entity_id
              formula_weight
              pdbx_number_of_molecules
              pdbx_formula_weight_exptl
              rcsb_nonpolymer_entity_container_identifiers {
                chem_comp_id
                auth_asym_ids
                auth_seq_ids
              }
              rcsb_nonpolymer_entity {
                pdbx_chem_comp_descriptor {
                  type
                  descriptor
                }
                rcsb_chem_comp_descriptor {
                  comp_id
                  id
                  name
                  smiles
                  inchi
                  inchikey
                  formula
                }
              }
            }
            rcsb_binding_affinity {
              comp_id
              value
              unit
              type
            }
            rcsb_entry_info {
              nonpolymer_entity_count
            }
          }
        }
        """
        
        # Make the GraphQL request
        graphql_response = requests.post(
            url,
            json={"query": graphql_query, "variables": {"pdb_id": pdb_id}},
            headers=headers
        )
        
        if graphql_response.status_code != 200:
            return {
                'statusCode': graphql_response.status_code,
                'body': f'PDB API returned status code {graphql_response.status_code}: {graphql_response.text}'
            }
        
        data = graphql_response.json()
        entry_data = data.get('data', {}).get('entry')
        
        if not entry_data:
            return {
                'statusCode': 404,
                'body': f'PDB ID {pdb_id} not found'
            }
        
        # Extract ligand information
        ligands = []
        nonpolymer_entities = entry_data.get('nonpolymer_entity', [])
        
        for entity in nonpolymer_entities:
            ligand_info = {
                'description': entity.get('pdbx_description'),
                'entity_id': entity.get('pdbx_entity_id'),
                'formula_weight': entity.get('formula_weight'),
                'molecules_count': entity.get('pdbx_number_of_molecules')
            }
            
            # Extract chemical identifiers
            container = entity.get('rcsb_nonpolymer_entity_container_identifiers', {})
            if container:
                ligand_info['chem_comp_id'] = container.get('chem_comp_id')
                ligand_info['chain_ids'] = container.get('auth_asym_ids')
                ligand_info['residue_numbers'] = container.get('auth_seq_ids')
            
            # Extract chemical descriptors (SMILES, InChI, etc.)
            nonpolymer_data = entity.get('rcsb_nonpolymer_entity', {})
            if nonpolymer_data:
                chem_comp = nonpolymer_data.get('rcsb_chem_comp_descriptor', {})
                if chem_comp:
                    ligand_info['smiles'] = chem_comp.get('smiles')
                    ligand_info['inchi'] = chem_comp.get('inchi')
                    ligand_info['formula'] = chem_comp.get('formula')
                    ligand_info['name'] = chem_comp.get('name')
            
            # Add links to ligand viewers
            if 'chem_comp_id' in ligand_info:
                ligand_id = ligand_info['chem_comp_id']
                ligand_info['links'] = {
                    'ligand_page': f"https://www.rcsb.org/ligand/{ligand_id}",
                    'structure_view': f"https://www.rcsb.org/structure/{pdb_id}"
                }
            
            ligands.append(ligand_info)
        
        # Extract binding affinity data if available
        binding_affinities = []
        for affinity in entry_data.get('rcsb_binding_affinity', []):
            binding_affinities.append({
                'ligand_id': affinity.get('comp_id'),
                'value': affinity.get('value'),
                'unit': affinity.get('unit'),
                'type': affinity.get('type')
            })
        
        result = {
            'pdb_id': pdb_id,
            'title': entry_data.get('struct', {}).get('title'),
            'ligand_count': entry_data.get('rcsb_entry_info', {}).get('nonpolymer_entity_count'),
            'ligands': ligands
        }
        
        if binding_affinities:
            result['binding_affinities'] = binding_affinities
        
        return result
        
    except Exception as e:
        return {
            'statusCode': 500,
            'body': str(e)
        }

Requirements.txt

You'll need to include a requirements.txt file for your Lambda layers:

requests==2.31.0

Integration with Amazon Bedrock Agents

To integrate these tools with Amazon Bedrock Agents:

    Deploy the CloudFormation template
    Create an API schema for each function
    Create an Action Group in your Bedrock Agent
    Use the Lambda function ARNs from the CloudFormation outputs

For example, for the search_proteins function:

{
  "openapi": "3.0.0",
  "info": {
    "title": "UniProt API",
    "version": "1.0.0"
  },
  "paths": {
    "/search": {
      "post": {
        "summary": "Search for proteins in the UniProt database",
        "operationId": "searchProteins",
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "required": ["query"],
                "properties": {
                  "query": {
                    "type": "string",
                    "description": "Search term or keyword"
                  },
                  "organism": {
                    "type": "string",
                    "description": "Limit search to a specific organism (e.g., 'human', 'mouse')"
                  },
                  "limit": {
                    "type": "integer",
                    "default": 10,
                    "description": "Maximum number of results to return"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful operation"
          }
        }
      }
    }
  }
}

The templates follow the same structure as the stock data example, making them compatible with Amazon Bedrock Agents. The Lambda functions are implemented to handle the direct invocation pattern used by Bedrock Agents.