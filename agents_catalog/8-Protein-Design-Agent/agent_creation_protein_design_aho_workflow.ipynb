{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Protein Design Agent with AWS HealthOmics Workflow Integration\n",
    "\n",
    "This notebook demonstrates how to create a Bedrock agent that can trigger AWS HealthOmics workflows for protein design optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "1. Go through the notebook environment setup in the agents_catalog/0-Notebook-environment/ folder\n",
    "\n",
    "2. Deploy protein_design_stack.yaml to your AWS account to instantiate a ECR repository with a custom Docker image, a AWS HealthOmics (AHO) private workflow, and a lambda function that invokes the AHO workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for deploying the CloudFormation stack:\n",
    "1. Create a S3 bucket for storing required files in the same region as your cf stack\n",
    "2. Upload workflow definition files to S3\n",
    "3. Package and upload container code to S3\n",
    "4. Deploy the CloudFormation stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Function to create S3 bucket in specified region\n",
    "def create_s3_bucket(bucket_name, region):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in the specified region if it doesn't exist\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_name (str): Name of the S3 bucket to create\n",
    "    region (str): AWS region where the bucket should be created\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if bucket was created or already exists, False otherwise\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Check if bucket already exists\n",
    "        response = s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"404\" in str(e):\n",
    "            # Bucket doesn't exist, create it\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    # Special case for us-east-1 which doesn't accept LocationConstraint\n",
    "                    response = s3_client.create_bucket(\n",
    "                        Bucket=bucket_name\n",
    "                    )\n",
    "                else:\n",
    "                    response = s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={\n",
    "                            'LocationConstraint': region\n",
    "                        }\n",
    "                    )\n",
    "                print(f\"Successfully created bucket {bucket_name} in {region}\")\n",
    "                return True\n",
    "            except Exception as create_error:\n",
    "                print(f\"Error creating bucket: {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Error checking bucket: {e}\")\n",
    "            return False\n",
    "\n",
    "# Create zip file of container code\n",
    "def create_container_zip():\n",
    "    try:\n",
    "        shutil.make_archive('code', 'zip', 'container')\n",
    "        print(\"Successfully created code.zip from container directory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating zip file: {e}\")\n",
    "\n",
    "# Upload workflow files and container code to S3\n",
    "def upload_to_s3(bucket_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Upload workflow files\n",
    "    workflow_files = ['main.nf', 'nextflow.config', 'config.yaml', 'parameter-template.json']\n",
    "    for file in workflow_files:\n",
    "        try:\n",
    "            s3.upload_file(\n",
    "                f'aho_workflow/{file}', \n",
    "                bucket_name, \n",
    "                f'workflow/{file}'\n",
    "            )\n",
    "            print(f\"Uploaded {file} to s3://{bucket_name}/workflow/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file}: {e}\")\n",
    "    \n",
    "    # Upload container code zip\n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            'code.zip',\n",
    "            bucket_name,\n",
    "            'code.zip'\n",
    "        )\n",
    "        print(f\"Uploaded code.zip to s3://{bucket_name}/\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading code.zip: {e}\")\n",
    "\n",
    "# Define the CloudFormation parameters\n",
    "def write_cf_parameters(bucket_name):\n",
    "    '''Write the param JSON file for creating the cf stack'''\n",
    "    cf_parameters = [\n",
    "        {\n",
    "            \"ParameterKey\": \"S3BucketName\",\n",
    "            \"ParameterValue\": bucket_name\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"StackPrefix\",\n",
    "            \"ParameterValue\": \"protein-design\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"ApplicationName\",\n",
    "            \"ParameterValue\": \"HealthOmics-Workflow\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"WorkflowPath\",\n",
    "            \"ParameterValue\": \"workflow\"  # Default value from the template\n",
    "        },\n",
    "        {\n",
    "            \"ParameterKey\": \"SecretName\",\n",
    "            \"ParameterValue\": \"protein-design-secret\"  # Default value from the template\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Write parameters to cf_parameter.json file\n",
    "    with open('cf_parameters.json', 'w') as f:\n",
    "        json.dump(cf_parameters, f, indent=2)\n",
    "\n",
    "    print(f\"CloudFormation parameters written to cf_parameters.json\")\n",
    "    print(f\"File path: {os.path.abspath('cf_parameters.json')}\")\n",
    "\n",
    "# Main execution\n",
    "REGION = \"us-west-2\"  # Change this to your desired region\n",
    "s3_bucket_name = \"hcls-bedrock-agents-byot-aho-20240416-west2\"  # Base bucket name\n",
    "\n",
    "# Create the S3 bucket if it doesn't exist\n",
    "bucket_created = create_s3_bucket(s3_bucket_name, REGION)\n",
    "\n",
    "if bucket_created:\n",
    "    # Create and write CloudFormation parameters\n",
    "    write_cf_parameters(s3_bucket_name)\n",
    "    \n",
    "    # Create zip and upload files\n",
    "    create_container_zip()\n",
    "    upload_to_s3(s3_bucket_name)\n",
    "else:\n",
    "    print(\"Failed to create or verify S3 bucket. CloudFormation parameters not written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS CLI commands to deploy the CloudFormation stack:\n",
    "````\n",
    "# Deploy the CloudFormation stack\n",
    "aws cloudformation create-stack \\\n",
    "    --stack-name hcls-bedrock-agents-byot-aho-stack-20240416-lg \\\n",
    "    --template-body file://protein_design_stack.yaml \\\n",
    "    --parameters file://cf_parameters.json \\\n",
    "    --capabilities CAPABILITY_IAM CAPABILITY_AUTO_EXPAND CAPABILITY_NAMED_IAM \\\n",
    "    --region us-west-2\n",
    "\n",
    "# Monitor stack creation\n",
    "aws cloudformation describe-stacks \\\n",
    "    --stack-name hcls-bedrock-agents-byot-aho-stack-20240416-lg \\\n",
    "    --query 'Stacks[0].StackStatus'\n",
    "\n",
    "# Get stack outputs once complete\n",
    "aws cloudformation describe-stacks \\\n",
    "    --stack-name hcls-bedrock-agents-byot-aho-stack-20240416-lg \\\n",
    "    --query 'Stacks[0].Outputs'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in environment variables to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve import path\n",
    "%store -r IMPORTS_PATH\n",
    "\n",
    "# Retrieve account info\n",
    "%store -r account_id\n",
    "%store -r region\n",
    "\n",
    "# Retrieve model lists\n",
    "%store -r agent_foundation_model\n",
    "\n",
    "%run $IMPORTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS clients and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Configure AWS clients\n",
    "session = boto3.Session()\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "bedrock = boto3.client('bedrock', REGION)\n",
    "cfn = boto3.client('cloudformation', REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CloudFormation Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "STACK_NAME = 'hcls-bedrock-agents-byot-aho-stack-20240416-lg'\n",
    "\n",
    "# Initialize the CloudFormation client with the specific region\n",
    "cloudformation = boto3.client('cloudformation', region_name=REGION)\n",
    "\n",
    "def get_cloudformation_outputs(stack_name):\n",
    "    try:\n",
    "        response = cloudformation.describe_stacks(StackName=stack_name)\n",
    "        outputs = {}\n",
    "        for output in response['Stacks'][0]['Outputs']:\n",
    "            outputs[output['OutputKey']] = output['OutputValue']\n",
    "        return outputs\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting CloudFormation outputs: {e}\")\n",
    "        raise\n",
    "\n",
    "# Get the outputs from CloudFormation\n",
    "cf_outputs = get_cloudformation_outputs(STACK_NAME)\n",
    "print(\"CloudFormation Outputs:\")\n",
    "print(json.dumps(cf_outputs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_function_arn = cf_outputs[\"TriggerFunctionArn\"]\n",
    "lambda_function_name = \"hcls-bedrock-agents-byot-a-WorkflowTriggerFunction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent configuration\n",
    "agent_name = 'ProteinDesignAgent'\n",
    "agent_description = \"Agent for protein design using HealthOmics workflow\"\n",
    "agent_instruction = \"\"\"You are an expert in protein design and optimization using AWS HealthOmics workflows. \n",
    "Your primary task is to help users run protein design optimization workflows and provide relevant insights.\n",
    "\n",
    "When providing your response:\n",
    "a. Start with a brief summary of your understanding of the user's query.\n",
    "b. Explain the steps you're taking to address the query. Ask for clarifications from the user if required.\n",
    "c. Present the results of the workflow execution.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate agent with the desired configuration\n",
    "agents = AgentsForAmazonBedrock()\n",
    "\n",
    "protein_design_agent = agents.create_agent(\n",
    "    agent_name,\n",
    "    agent_description,\n",
    "    agent_instruction,\n",
    "    agent_foundation_model,\n",
    "    code_interpretation=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Extract useful agent information\n",
    "protein_design_agent_id = protein_design_agent[0]\n",
    "protein_design_agent_arn = f\"arn:aws:bedrock:{REGION}:{account_id}:agent/{protein_design_agent_id}\"\n",
    "\n",
    "print(f\"Agent created with ID: {protein_design_agent_id}\")\n",
    "print(f\"Agent ARN: {protein_design_agent_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Action Group Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_defs = [\n",
    "    {\n",
    "        \"name\": \"trigger_aho_workflow\",\n",
    "        \"description\": \"Trigger the AWS HealthOmics workflow for protein design optimization\",\n",
    "        \"parameters\": {\n",
    "            \"workflowId\": {\n",
    "                \"description\": \"The ID of the HealthOmics workflow to run\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"runName\": {\n",
    "                \"description\": \"Name for the workflow run\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"container_image\": {\n",
    "                \"description\": \"ECR image URI for the protein design container\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"seed_sequence\": {\n",
    "                \"description\": \"The input protein sequence to optimize\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"outputUri\": {\n",
    "                \"description\": \"S3 URI where the workflow outputs will be stored\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"roleArn\": {\n",
    "                \"description\": \"ARN of the IAM role for workflow execution\",\n",
    "                \"required\": True,\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"requireConfirmation\": \"DISABLED\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Action Group with Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add action group with Lambda function\n",
    "agents.add_action_group_with_lambda(\n",
    "    agent_name=agent_name,\n",
    "    lambda_function_name=lambda_function_name,\n",
    "    source_code_file=lambda_function_arn,\n",
    "    agent_action_group_name=\"ProteinDesignActions\",\n",
    "    agent_action_group_description=\"Actions for protein design using AWS HealthOmics workflows\",\n",
    "    agent_functions=function_defs,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Lambda Resource-Based Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda', REGION)\n",
    "\n",
    "try:\n",
    "    # Add the new statement to the existing policy\n",
    "    response = lambda_client.add_permission(\n",
    "        FunctionName=lambda_function_arn,\n",
    "        StatementId=\"AllowBedrockAgentAccess\",\n",
    "        Action=\"lambda:InvokeFunction\",\n",
    "        Principal=\"bedrock.amazonaws.com\",\n",
    "        SourceArn=protein_design_agent_arn\n",
    "    )\n",
    "    \n",
    "    print(\"Resource policy added successfully.\")\n",
    "    print(\"Response:\", response)\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    print(\"Permission already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding permission: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent alias\n",
    "protein_design_agent_alias_id, protein_design_agent_alias_arn = agents.create_agent_alias(\n",
    "    protein_design_agent[0], 'v1'\n",
    ")\n",
    "\n",
    "# Store the alias ARN for future use\n",
    "%store protein_design_agent_alias_arn\n",
    "\n",
    "print(f\"Agent alias created with ID: {protein_design_agent_alias_id}\")\n",
    "print(f\"Agent alias ARN: {protein_design_agent_alias_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", REGION)\n",
    "session_id = str(uuid.uuid1())\n",
    "\n",
    "test_query = \"Please tell me what you can help me do to optimize a protein sequence\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.invoke_agent(\n",
    "    inputText=test_query,\n",
    "    agentId=protein_design_agent_id,\n",
    "    agentAliasId=protein_design_agent_alias_id,\n",
    "    sessionId=session_id,\n",
    "    enableTrace=True\n",
    ")\n",
    "\n",
    "print(\"Request sent to Agent:\\n{}\".format(response))\n",
    "print(\"====================\")\n",
    "print(\"Agent processing query now\")\n",
    "print(\"====================\")\n",
    "\n",
    "# Initialize an empty string to store the answer\n",
    "answer = \"\"\n",
    "\n",
    "# Iterate through the event stream\n",
    "for event in response['completion']:\n",
    "    # Check if the event is a 'chunk' event\n",
    "    if 'chunk' in event:\n",
    "        chunk_obj = event['chunk']\n",
    "        if 'bytes' in chunk_obj:\n",
    "            # Decode the bytes and append to the answer\n",
    "            chunk_data = chunk_obj['bytes'].decode('utf-8')\n",
    "            answer += chunk_data\n",
    "\n",
    "print(\"Agent Answer: {}\".format(answer))\n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
